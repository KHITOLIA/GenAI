import streamlit as st
from langchain.llms import Ollama
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
import os
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
import hashlib
import shutil



llm = Ollama(model = 'llama3.2')
memory = ConversationBufferMemory(return_messages=True)
conversation = ConversationChain(llm = llm, memory = memory)

data_path = 'Data/'
os.makedirs(data_path, exist_ok=True)

db_faiss_path = 'Faiss/'
os.makedirs(db_faiss_path, exist_ok=True)

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Utility: Generate hash for a file (used to identify unique uploads)
def get_file_hash(file_path):
    with open(file_path, "rb") as f:
        return hashlib.md5(f.read()).hexdigest()

# Load and split document
def load_data(data_path):
    ext = data_path.split('.')[-1].lower()
    if ext == 'pdf':
        loader = PyMuPDFLoader(data_path)
    else:
        raise ValueError(f"Unsupported file format: {ext}")
    return loader.load_and_split()

# Split into chunks
def create_chunks(extracted_data):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500, 
        chunk_overlap=50,
        length_function=len
    )
    text_chunks = text_splitter.split_documents(extracted_data)
    return text_chunks

# Embed and store in FAISS
def create_embeddings(text_chunks):
    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    vectorstore = FAISS.from_documents(text_chunks, embedding=embeddings)
    return vectorstore

# Load existing FAISS vector DB
def load_vector_store(vector_dir):
    embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    db = FAISS.load_local(vector_dir, embeddings=embedding_model, allow_dangerous_deserialization=True)
    return db


# Streamlit UI
st.set_page_config(page_title="Jarvis Chatbot", page_icon=":robot_face:", layout="wide")
st.title("Jarvis Chatbot :robot_face:")
st.write('Welcome to the Jarvis Chatbot! Ask me anything : ')


uploaded_file = st.file_uploader("Choose a PDF file", type=["pdf", "txt", "docx", "csv"])
if uploaded_file is not None:
    # Save uploaded file to disk
    file_path = os.path.join(data_path, uploaded_file.name)
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    # st.success(f"File '{uploaded_file.name}' uploaded successfully!")

    # Generate unique hash and create vector DB folder for this file
    file_hash = get_file_hash(file_path)
    vector_dir = os.path.join(db_faiss_path, file_hash)
    os.makedirs(vector_dir, exist_ok=True)

    if not os.listdir(vector_dir):  # If DB for this file doesn't exist
        try:
            documents = load_data(data_path=file_path)
            # st.success('‚úÖ Loading the documents')
            text_chunks = create_chunks(extracted_data=documents)
            # st.success("‚úÖ Creating the text chunks")
            vectorstore = create_embeddings(text_chunks=text_chunks)
            # st.success("‚úÖ Creating the vector database")
            vectorstore.save_local(vector_dir)
            st.markdown(f"‚úÖ Vector Database saved successfully at {vector_dir}!")   
        except Exception as e:
            st.error(f"‚ùå Error processing the file: {e}")
    else:
        pass
        # st.success("‚ÑπÔ∏è Vector database already exists for this file. Skipping creation.")

    # Store the vector path for retrieval
    st.session_state["vector_path"] = vector_dir

user_input = st.chat_input("Type your message... ")

if user_input:
    if "vector_path" not in st.session_state:
        st.warning("Upload a file first.")
    else:
        st.session_state.chat_history.append({"role": "user", "content": user_input})
        try:
            db = load_vector_store(st.session_state['vector_path'])
            docs = db.similarity_search(user_input, k=1)[0].page_content
            prompt = f"""Hi, your name is Hr_bot generated by Tushar Khitoliya. No irrelevant info. 
            Query: {user_input}
            Context: {docs}

            Provide the exact information in words only. Highlight the relevant information. 
            Please provide the answer in bullet points focusing on main topics. 
            After providing the information, ask for the next query."""
            response = llm.invoke(prompt)
            
            # Now append the assistant's response only once
            st.session_state.chat_history.append({"role": "assistant", "content": response})
        except Exception as e:
            err = f"‚ùå Error: {str(e)}"
            st.session_state.chat_history.append({"role": "assistant", "content": err})

import datetime

for msg in st.session_state.chat_history:
    role = msg["role"]
    content = msg["content"]

    # Add emoji & timestamp
    timestamp = datetime.datetime.now().strftime("%H:%M:%S")
    if role == "user":
        bubble_header = f"üë§ **You**  _(at {timestamp})_"
        bubble_color = "#DCF8C6"  # WhatsApp-style green
    else:
        bubble_header = f"ü§ñ **Jarvis** _(at {timestamp})_"
        bubble_color = "#F1F0F0"  # WhatsApp-style gray

    with st.chat_message(role):
        st.markdown(
            f"""
            <div style="background-color: {bubble_color}; padding: 10px 15px; border-radius: 15px;">
                <p style="margin: 0; color: black;">{bubble_header}</p>
                <hr style="margin: 4px 0;">
                <div style="color: black;">{content}</div>
            </div>
            """,
            unsafe_allow_html=True
        )

# Stop and cleanup
if st.button("üõë Reset"):
    try:
        shutil.rmtree(db_faiss_path)
        shutil.rmtree(data_path)
        st.session_state.clear()
        st.success("‚úÖ Application reset.")
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Cleanup failed: {e}")
    os._exit(0)    