from langchain_community.document_loaders import PyPDFLoader , DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

data_path = "data/"

# Step 1 : load the RAW PDF

def load_data(data_path):
     loader = DirectoryLoader(data_path, glob = "*.pdf", loader_cls = PyPDFLoader)
     documents = loader.load()
     return documents

documents = load_data(data_path)
print("number of documents loaded : ", len(documents))

# Step 2: Create Chunks of Text
# Step 2: Create Chunks
def create_chunks(extracted_data):
    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,
                                                 chunk_overlap=50)
    text_chunks=text_splitter.split_documents(extracted_data)
    return text_chunks

text_chunks=create_chunks(extracted_data=documents)
print("Length of Text Chunks: ", len(text_chunks))

#step 3: create Embeddings

def create_embeddings(text_chunks):
     embeddings = HuggingFaceEmbeddings(model_name = "sentence-transformers/all-MiniLM-L6-v2")
     vectorstore = FAISS.from_documents(text_chunks, embeddings)
     return vectorstore

embedding_model = create_embeddings(text_chunks)
db_faiss_path = "database/faiss_database"
embedding_model.save_local(db_faiss_path)
print("FAISS database saved at: ", db_faiss_path) 